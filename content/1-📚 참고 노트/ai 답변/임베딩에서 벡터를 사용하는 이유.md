---
publish: true
---

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 임베딩에서 벡터를 사용하는 이유

임베딩(Embedding)은 단어·문장·아이템 등 이산적(discrete) 데이터 요소를 연속적(continuous)인 고차원 공간상의 점으로 대응(mapping)하는 기법이다. 이때 벡터를 사용하는 이유는 다음과 같다.

## 1. 연속 공간 표현의 유연성

이산적 토큰(token)에는 순서나 거리 개념이 없으나, 벡터 공간에서는 유클리드 거리·코사인 유사도 등을 통해 **유사성(similarity)**을 연산으로 직접 측정할 수 있다.

- 예: 비슷한 의미의 단어일수록 벡터 간 거리가 가깝다.
- 결과적으로 기계 학습 모델은 데이터 간 관계를 **수치화**된 거리로 학습 가능하다.


## 2. 차원 축소 및 정보 압축

고차원 희소 표현(one-hot) 대신, 수백~수천 차원의 연속 벡터를 사용함으로써 정보의 **밀집 표현(dense representation)**이 가능하다.

- 희소 벡터(one-hot)
    - 차원 수 = 어휘 크기(vocabulary size)
    - 대부분 요소가 0인 반면 단 하나만 1
- 임베딩 벡터
    - 상대적으로 낮은 차원(e.g. 100~1,000)
    - 각 차원에 의미 있는 분산 정보 분배


## 3. 선형 연산을 통한 의미 결합

벡터 간 덧셈·뺄셈·스케일링 같은 선형 대수 연산으로 **의미 결합(compositionality)**이 가능하다.

- “king − man + woman ≈ queen” 같은 구문 유사도 연산
- 문장·문단 임베딩 시 단어 벡터의 평균·가중합 계산


## 4. 신경망과의 자연스러운 연동

딥러닝 모델은 입력층부터 출력층까지 **행렬곱(matrix multiplication)** 기반으로 작동하므로, 임베딩 벡터를 가중치 행렬로 매핑(mapping)하고 역전파(backpropagation)로 학습하기에 적합하다.

- 임베딩 레이어는 단순 lookup table이지만, 학습 과정에서 벡터 요소가 조정되어 데이터 특성 학습
- 모델 전체의 파라미터 수를 유연하게 조절 가능


## 5. 일반화 및 전이학습

벡터 공간에 배치된 임베딩은 **전이학습(transfer learning)**이 가능하도록 만든다.

- 사전 학습된(pre-trained) 임베딩(Word2Vec, GloVe, BERT 등)을 다양한 다운스트림 과제에 활용
- 신규 태스크에서도 비슷한 의미의 단어·문장 벡터를 재사용하여 **일반화 성능** 향상


### 결론

임베딩에서 벡터를 사용하는 핵심 이유는

1. **유사성 측정**을 위한 연속 공간 제공
2. **밀집 표현**으로 차원 축소 및 정보 압축
3. **선형 연산**을 통한 의미 연산 가능
4. **딥러닝 연동**에 최적화된 구조
5. **전이학습**을 통한 효율적 지식 재활용

이러한 특성 덕분에 벡터 임베딩은 자연어 처리, 추천 시스템, 컴퓨터 비전 등 다양한 분야에서 기초적인 표현 학습 수단으로 자리 잡고 있다.

